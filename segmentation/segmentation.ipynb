{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satellite image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.gluon as gluon\n",
    "import mxnet.gluon.nn as nn\n",
    "\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "from mxnet.gluon.loss import Loss\n",
    "from mxnet import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave, imread\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopedia_layers = {'tulip_field_2016':'ttl1904', 'tulip_field_2017':'ttl1905'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithMaskDataset(dataset.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for loading images (with masks).\n",
    "    Based on: mxnet.incubator.apache.org/tutorials/python/data_augmentation_with_masks.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Path to root directory.\n",
    "    transform : callable, default None\n",
    "        A function that takes data and label and transforms them:\n",
    "    ::\n",
    "        transform = lambda data, label: (data.astype(np.float32)/255, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self._root = os.path.expanduser(root)\n",
    "        self._transform = transform\n",
    "        self._exts = ['.png']\n",
    "        self._list_images(self._root)\n",
    "        self._mask_fn = \n",
    "\n",
    "    def _list_images(self, root):\n",
    "        images = collections.defaultdict(dict)\n",
    "        for filename in sorted(os.listdir(root)):\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            mask_flag = name.contains(\"geopedia\")\n",
    "            if ext.lower() not in self._exts:\n",
    "                continue\n",
    "            if not mask_flag:\n",
    "                patch_id = filename.split('_')[1]\n",
    "                year = datetime.strptime(filename.split('_')[3], \"%Y%m%d-%H%M%S\").year\n",
    "                mask_fn = 'tulip_{}_geopedia_{}.png'.format(patch_id, geopedia_layers['tulip_field_{}'.format(year)])\n",
    "                images[name][\"base\"] = filename\n",
    "                images[name][\"mask\"] = mask_fn\n",
    "        self._image_list = list(images.values())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 'base' in self._image_list[idx], \"Couldn't find base image for: \" + image_list[idx][\"mask\"]\n",
    "        base_filepath = os.path.join(self._root, self._image_list[idx][\"base\"])\n",
    "        base = mx.image.imread(base_filepath)\n",
    "        assert 'mask' in self._image_list[idx], \"Couldn't find mask image for: \" + image_list[idx][\"base\"]\n",
    "        mask_filepath = os.path.join(self._root, self._image_list[idx][\"mask\"])\n",
    "        mask = mx.image.imread(mask_filepath)\n",
    "        if self._transform is not None:\n",
    "            return self._transform(base, mask)\n",
    "        else:\n",
    "            return base, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_augmentation(joint):\n",
    "    # Random crop\n",
    "    crop_height = 224\n",
    "    crop_width = 224\n",
    "    aug = mx.image.RandomCropAug(size=(crop_width, crop_height)) # Watch out: weight before height in size param!\n",
    "    aug_joint = aug(joint)\n",
    "    # Deterministic resize\n",
    "    resize_size = 100\n",
    "    aug = mx.image.ResizeAug(resize_size)\n",
    "    aug_joint = aug(aug_joint)\n",
    "    # Add more translation/scale/rotation augmentations here...\n",
    "    return aug_joint\n",
    "\n",
    "\n",
    "def color_augmentation(base):\n",
    "    # Only applied to the base image, and not the mask layers.\n",
    "    aug = mx.image.BrightnessJitterAug(brightness=0.2)\n",
    "    aug_base = aug(base)\n",
    "    # Add more color augmentations here...\n",
    "    return aug_base\n",
    "\n",
    "\n",
    "def joint_transform(base, mask):\n",
    "    ### Convert types\n",
    "    base = base.astype('float32')/255\n",
    "    mask = mask.astype('float32')/255\n",
    "    \n",
    "    ### Join\n",
    "    # Concatinate on channels dim, to obtain an 6 channel image\n",
    "    # (3 channels for the base image, plus 3 channels for the mask)\n",
    "    base_channels = base.shape[2] # so we know where to split later on\n",
    "    joint = mx.nd.concat(base, mask, dim=2)\n",
    "\n",
    "    ### Augmentation Part 1: positional\n",
    "    aug_joint = positional_augmentation(joint)\n",
    "    \n",
    "    ### Split\n",
    "    aug_base = aug_joint[:, :, :base_channels]\n",
    "    aug_mask = aug_joint[:, :, base_channels:]\n",
    "    \n",
    "    ### Augmentation Part 2: color\n",
    "    aug_base = color_augmentation(aug_base)\n",
    "\n",
    "    return aug_base, aug_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IoU metric tends to have a \"squaring\" effect on the errors relative to the Dice score (aka F score). So the F score tends to measure something closer to average performance, while the IoU score measures something closer to the worst case performance.\n",
    "\n",
    "https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not very confident on the quality of the training data/ground truth, lets go for the Dice coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = mx.sym.sum(mx.sym.broadcast_mul(y_true, y_pred), axis=(1, 2, 3))\n",
    "    return mx.sym.broadcast_div((2. * intersection + 1.),(mx.sym.sum(y_true, axis=(1, 2, 3)) + mx.sym.sum(y_pred, axis=(1, 2, 3)) + 1.))\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    intersection = mx.sym.sum(mx.sym.broadcast_mul(y_true, y_pred), axis=1, )\n",
    "    return -mx.sym.broadcast_div((2. * intersection + 1.),(mx.sym.broadcast_add(mx.sym.sum(y_true, axis=1), mx.sym.sum(y_pred, axis=1)) + 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(data, filters, kernel_size):\n",
    "    '''\n",
    "    Returns a convolutional block composed of a convolutional layer followed\n",
    "    by batch normalization and a ReLu activation.\n",
    "    '''\n",
    "    conv = mx.sym.Convolution(data, num_filter=filters, kernel=(kernel_size,kernel_size), pad=(1,1))\n",
    "    conv = mx.sym.BatchNorm(conv)\n",
    "    conv = mx.sym.Activation(conv, act_type='relu')\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(data, filters):\n",
    "    '''\n",
    "    Returns two consecutive convolutional blocks\n",
    "    '''\n",
    "    out = conv_block(data, filters, 3)\n",
    "    out = conv_block(out,  filters, 3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(data, concat, filters):\n",
    "    deconv = mx.sym.Deconvolution(data, num_filter=filters, kernel=(2,2), stride=(1,1), no_bias=True)\n",
    "    out = mx.sym.concat(*[deconv, concat], dim=1)\n",
    "    out = conv_block(out, filters, 3)\n",
    "    out = conv_block(out, filters, 3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet():\n",
    "    # Inputs\n",
    "    data = mx.sym.Variable(name='data')\n",
    "    label = mx.sym.Variable(name='label')\n",
    "    \n",
    "    # Down blocks\n",
    "    down1 = down_block(data, 32)\n",
    "    pool1 = mx.sym.Pooling(down1, kernel=(2,2), pool_type='max')\n",
    "    \n",
    "    down2 = down_block(pool1, 64)\n",
    "    pool2 = mx.sym.Pooling(down2, kernel=(2,2), pool_type='max')\n",
    "    \n",
    "    down3 = down_block(pool2, 128)\n",
    "    pool3 = mx.sym.Pooling(down3, kernel=(2,2), pool_type='max')\n",
    "    \n",
    "    down4 = down_block(pool3, 256)\n",
    "    pool4 = mx.sym.Pooling(down4, kernel=(2,2), pool_type='max')\n",
    "    \n",
    "    down5 = down_block(pool4, 512)\n",
    "    \n",
    "    # Up blocks\n",
    "    up4 = up_block(down5,down4, 256)\n",
    "    up3 = up_block(up4,  down3, 128)\n",
    "    up2 = up_block(up3,  down2, 64)\n",
    "    up1 = up_block(up1,  down1, 32)\n",
    "    \n",
    "    # Final layers\n",
    "    conv = mx.sym.Convolution(up1, num_filter=1, kernel=(1,1), name='conv10_1')\n",
    "    conv = mx.sym.sigmoid(conv, name='softmax')\n",
    "    \n",
    "    net = mx.sym.Flatten(conv)\n",
    "    loss = mx.sym.MakeLoss(dice_coef_loss(label, net), normalization='batch')\n",
    "    mask_output = mx.sym.BlockGrad(conv, 'mask')\n",
    "    out = mx.sym.Group([loss, mask_output])\n",
    "    \n",
    "#     return mx.sym.Custom(net, pos_grad_scale = pos, neg_grad_scale = neg, name = 'softmax', op_type = 'weighted_logistic_regression')\n",
    "#     return mx.sym.LogisticRegressionOutput(net, name='softmax')\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
