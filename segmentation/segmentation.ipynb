{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satellite image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet.gluon as gluon\n",
    "import mxnet.gluon.nn as nn\n",
    "\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "from mxnet.gluon.loss import Loss\n",
    "from mxnet import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave, imread\n",
    "from datetime import datetime\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopedia_layers = {'tulip_field_2016':'ttl1904', 'tulip_field_2017':'ttl1905'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithMaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for loading images (with masks).\n",
    "    Based on: mxnet.incubator.apache.org/tutorials/python/data_augmentation_with_masks.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Path to root directory.\n",
    "    transform : callable, default None\n",
    "        A function that takes data and label and transforms them:\n",
    "    ::\n",
    "        transform = lambda data, label: (data.astype(np.float32)/255, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self._root = os.path.expanduser(root)\n",
    "        self._transform = transform\n",
    "        self._exts = ['.png']\n",
    "        self._list_images(self._root)\n",
    "\n",
    "    def _list_images(self, root):\n",
    "        images = collections.defaultdict(dict)\n",
    "        for filename in sorted(os.listdir(root)):\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            mask_flag = \"geopedia\" in name\n",
    "            if ext.lower() not in self._exts:\n",
    "                continue\n",
    "            if not mask_flag:\n",
    "                patch_id = filename.split('_')[1]\n",
    "                year = datetime.strptime(filename.split('_')[3], \"%Y%m%d-%H%M%S\").year\n",
    "                mask_fn = 'tulip_{}_geopedia_{}.png'.format(patch_id, geopedia_layers['tulip_field_{}'.format(year)])\n",
    "                images[name][\"base\"] = filename\n",
    "                images[name][\"mask\"] = mask_fn\n",
    "        self._image_list = list(images.values())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 'base' in self._image_list[idx], \"Couldn't find base image for: \" + image_list[idx][\"mask\"]\n",
    "        base_filepath = os.path.join(self._root, self._image_list[idx][\"base\"])\n",
    "        base = mx.image.imread(base_filepath)\n",
    "        assert 'mask' in self._image_list[idx], \"Couldn't find mask image for: \" + image_list[idx][\"base\"]\n",
    "        mask_filepath = os.path.join(self._root, self._image_list[idx][\"mask\"])\n",
    "        mask = mx.image.imread(mask_filepath, flag=0)\n",
    "        if self._transform is not None:\n",
    "            return self._transform(base, mask)\n",
    "        else:\n",
    "            return base, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_augmentation(joint):\n",
    "    # Random crop\n",
    "    crop_height = img_height\n",
    "    crop_width  = img_width\n",
    "    aug = mx.image.RandomCropAug(size=(crop_width, crop_height)) # Watch out: weight before height in size param!\n",
    "    aug_joint = aug(joint)\n",
    "    # Horizontal flip (50% probability)\n",
    "    aug = mx.image.HorizontalFlipAug(0.5)\n",
    "    aug_joint = aug(aug_joint)\n",
    "    return aug_joint\n",
    "\n",
    "\n",
    "def color_augmentation(base):\n",
    "    # Only applied to the base image, and not the mask layers.\n",
    "    aug = mx.image.ColorJitterAug(brightness=0.2, contrast=0.2, saturation=0)\n",
    "    aug_base = aug(base)\n",
    "    return aug_base\n",
    "\n",
    "\n",
    "def joint_transform(base, mask):\n",
    "    ### Convert types\n",
    "    base = base.astype('float32')/255\n",
    "    mask = mask.astype('float32')/255\n",
    "    \n",
    "    ### Join\n",
    "    # Concatinate on channels dim, to obtain an 6 channel image\n",
    "    # (3 channels for the base image, plus 3 channels for the mask)\n",
    "    base_channels = base.shape[2] # so we know where to split later on\n",
    "    joint = mx.nd.concat(base, mask, dim=2)\n",
    "\n",
    "    ### Augmentation Part 1: positional\n",
    "    aug_joint = positional_augmentation(joint)\n",
    "    \n",
    "    ### Split\n",
    "    aug_base = aug_joint[:, :, :base_channels]\n",
    "    aug_mask = aug_joint[:, :, base_channels:]\n",
    "    \n",
    "    ### Augmentation Part 2: color\n",
    "    aug_base = color_augmentation(aug_base)\n",
    "    \n",
    "    # Reshape the tensors so the order is now (channels, w, h)\n",
    "    aug_base =  mx.nd.transpose(aug_base, (2,0,1))\n",
    "    aug_mask =  mx.nd.transpose(aug_mask, (2,0,1))\n",
    "    \n",
    "    return aug_base, aug_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(channels, kernel_size):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(\n",
    "        nn.Conv2D(channels, kernel_size, padding=1, use_bias=False),\n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu')\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(channels):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(\n",
    "        conv_block(channels, 3),\n",
    "        conv_block(channels, 3)\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class up_block(nn.HybridBlock):\n",
    "    def __init__(self, channels, shrink=True, **kwargs):\n",
    "        super(up_block, self).__init__(**kwargs)\n",
    "        self.upsampler = nn.Conv2DTranspose(channels=channels, kernel_size=4, strides=2, \n",
    "                                            padding=1, use_bias=False)\n",
    "        self.conv1 = conv_block(channels, 1)\n",
    "        self.conv3_0 = conv_block(channels, 3)\n",
    "        if shrink:\n",
    "            self.conv3_1 = conv_block(int(channels/2), 3)\n",
    "        else:\n",
    "            self.conv3_1 = conv_block(channels, 3)\n",
    "    def hybrid_forward(self, F, x, s):\n",
    "        x = self.upsampler(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.Crop(*[x,s], center_crop=True)\n",
    "        x = s + x\n",
    "        x = self.conv3_0(x)\n",
    "        x = self.conv3_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Unet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.d0 = down_block(64)\n",
    "            \n",
    "            self.d1 = nn.HybridSequential()\n",
    "            self.d1.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(128))\n",
    "            \n",
    "            self.d2 = nn.HybridSequential()\n",
    "            self.d2.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(256))\n",
    "            \n",
    "            self.d3 = nn.HybridSequential()\n",
    "            self.d3.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(512))\n",
    "            \n",
    "            self.d4 = nn.HybridSequential()\n",
    "            self.d4.add(nn.MaxPool2D(2,2,ceil_mode=True), down_block(1024))\n",
    "            \n",
    "            self.u3 = up_block(512, shrink=True)\n",
    "            self.u2 = up_block(256, shrink=True)\n",
    "            self.u1 = up_block(128, shrink=True)\n",
    "            self.u0 = up_block(64, shrink=False)\n",
    "            \n",
    "            self.conv = nn.Conv2D(2,1)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x0 = self.d0(x)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.d2(x1)\n",
    "        x3 = self.d3(x2)\n",
    "        x4 = self.d4(x3)\n",
    "\n",
    "        y3 = self.u3(x4,x3)\n",
    "        y2 = self.u2(y3,x2)\n",
    "        y1 = self.u1(y2,x1)\n",
    "        y0 = self.u0(y1,x0)\n",
    "        \n",
    "        out = self.conv(y0)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IoU metric tends to have a \"squaring\" effect on the errors relative to the Dice score (aka F score). So the F score tends to measure something closer to average performance, while the IoU score measures something closer to the worst case performance.\n",
    "\n",
    "https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are not very confident on the quality of the training data/ground truth, lets go for the Dice coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    # TODO: Check if axis=[2,3] is correct (in the OG example it was axis=1)\n",
    "    intersection = nd.sum((y_true * y_pred), axis=[2,3])\n",
    "    return -(2. * intersection + 1.)/(nd.sum(y_true, axis=[2,3]) + nd.sum(y_pred, axis=[2,3]) + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeffLoss(Loss):\n",
    "    \"\"\"\n",
    "    Dice loss coefficient.\n",
    "    Input:\n",
    "        Tensor of size (batch_size, 1, width, height)\n",
    "    Output:\n",
    "        Loss tensor with shape (batch size) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smooth=1.0, _axis=[1,2,3], _weight = None, _batch_axis= 0, **kwards):\n",
    "        Loss.__init__(self, weight=_weight, batch_axis=_batch_axis, **kwards)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.smooth = smooth\n",
    "            self.axis = _axis\n",
    "\n",
    "    def hybrid_forward(self, F, label, pred):\n",
    "        assert label.shape[1] == 1, 'Incorrect dimensions for label: {}'.format(label.shape)\n",
    "        assert pred.shape[1]  == 1, 'Incorrect dimensions for prediction: {}'.format(pred.shape)\n",
    "        intersect = nd.sum((label * pred), axis=self.axis)\n",
    "        dice = -(2. * intersect + self.smooth)/(nd.sum(label, axis=self.axis) + nd.sum(pred, axis=self.axis) + self.smooth)\n",
    "        return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not in use right now, keeping it here in case it's useful in the future\n",
    "# def iou(a, b):\n",
    "#     i = nd.sum((a==1)*(b==1),axis=[2,3])\n",
    "#     u = nd.sum(a,axis=[2,3]) + nd.sum(b, axis=[2,3]) - i\n",
    "#     return (i+1)/(u+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeffMetric(mx.metric.EvalMetric):\n",
    "    \"\"\"Stores a moving average of the dice coeff\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiceCoeffMetric, self).__init__('DiceCoeff')\n",
    "        self.name = 'Dice coefficient'\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        override reset behavior\n",
    "        \"\"\"\n",
    "        self.num_inst = 0\n",
    "        self.sum_metric = 0.0\n",
    "\n",
    "    def update(self, loss):\n",
    "        \"\"\"\n",
    "        Implementation of updating metrics\n",
    "        \"\"\"\n",
    "        self.sum_metric += nd.sum(loss)\n",
    "        self.num_inst += loss.shape[0]\n",
    "\n",
    "        \n",
    "    def get(self):\n",
    "        \"\"\"Get the current evaluation result.\n",
    "        Override the default behavior\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        name : str\n",
    "           Name of the metric.\n",
    "        value : float\n",
    "           Value of the evaluation.\n",
    "        \"\"\"\n",
    "        value = (self.sum_metric / self.num_inst).asscalar() if self.num_inst != 0 else float('nan')\n",
    "        return (self.name, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_str(names, vals):\n",
    "    return '{}={}'.format(names, vals.asscalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(data_iterator, net):\n",
    "    dice = DiceCoeffLoss()\n",
    "    metric = DiceCoeffMetric()\n",
    "    for data, label in data_iterator:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        # data = color_normalize(data/255, mean, std)\n",
    "        output = net(data)\n",
    "        pred = nd.reshape(np.argmax(output, axis=1), (0, 1, 224, 224))\n",
    "        # prediction = mx.nd.argmax(output, axis=1)\n",
    "        loss = dice(label, pred)\n",
    "        metric.update(loss)\n",
    "    return metric.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util(net, train_iter, val_iter, loss_fn,\n",
    "               trainer, ctx, epochs, batch_size, checkpoint_dir, init_epoch=0):\n",
    "    '''\n",
    "    Function to train the neural network.\n",
    "    \n",
    "    PARAMS:\n",
    "    - net: network to train\n",
    "    - train_iter: gluon.data.DataLoader with the training data\n",
    "    - validation_iter: \"                      \"  validation data\n",
    "    - loss_fn: loss function to use for training\n",
    "    - trainer: gluon.Trainer to use for training\n",
    "    - ctx: context where we will operate (GPU or CPU)\n",
    "    - epochs: number of epochs to train for\n",
    "    - batch_size\n",
    "    - checkpoint_dir: directory where checkpoints are saved every 100 batches\n",
    "    - init_epoch: set to the initial epoch in case training is resumed from a previous execution\n",
    "    '''\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    res = {'train':[],'valid':[]}\n",
    "    for epoch in range(1 + init_epoch, epochs + init_epoch+1):\n",
    "        metric = DiceCoeffMetric()\n",
    "        for i, (data, label) in enumerate(train_iter):\n",
    "            st = time.time()\n",
    "            # Ensure context            \n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            # Normalize images?\n",
    "            # data = color_normalize(data/255, mean, std)\n",
    "            \n",
    "            with mx.autograd.record():\n",
    "                output = net(data)\n",
    "                pred = nd.reshape(np.argmax(output, axis=1), (0, 1, 224, 224))\n",
    "                loss = loss_fn(label, pred)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0], ignore_stale_grad=True)\n",
    "            \n",
    "            #  Keep a moving average of the losses\n",
    "            metric.update(loss)\n",
    "            names, vals = metric.get()\n",
    "            if i%5 == 0:\n",
    "                print('[Epoch %d Batch %d] speed: %f samples/s, training: %s'%(epoch, i, batch_size/(time.time()-st), metric_str(names, vals)))\n",
    "                           \n",
    "        net.save_params('%s/%d.params'%(checkpoint_dir, epoch))\n",
    "        names, train_acc = metric.get()\n",
    "        val_acc = evaluate_loss(val_iter, net)\n",
    "        res['train'].append(train_acc)\n",
    "        res['valid'].append(val_acc)\n",
    "        print(\"Epoch %s | training_acc %s | val_acc %s \" % (epoch, train_acc, val_acc))\n",
    "        metric.reset()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "img_width  = 224\n",
    "img_height = 224\n",
    "#data_shape = (batch_size, 1, img_width, img_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataLoader from our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdir = '../data/tulips/bloom/16/'\n",
    "train_ds = ImageWithMaskDataset(imgdir , transform=joint_transform)\n",
    "train_iter = gluon.data.DataLoader(train_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CREATE DIFFERENT TRAIN AND VAL SETS\n",
    "val_ds  = ImageWithMaskDataset(imgdir , transform=joint_transform)\n",
    "val_iter= gluon.data.DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mx_arrays(arrays):\n",
    "    \"\"\"\n",
    "    Array expected to be height x width x 3 (channels), and values are floats between 0 and 255.\n",
    "    \"\"\"\n",
    "    plt.subplots(figsize=(14, 8))\n",
    "    for idx, array in enumerate(arrays):\n",
    "        assert array.shape[2] == 3, \"RGB Channel should be last\"\n",
    "        plt.subplot(1, 2, idx+1)\n",
    "        plt.imshow(array.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = val_ds.__getitem__(222)\n",
    "sample_base = sample[0].astype('float32')\n",
    "sample_mask = sample[1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow((sample_base*sample_mask).asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_mx_arrays([sample_base, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = DiceCoeffLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', \n",
    "            {'learning_rate': 1e-4, 'beta1':0.9, 'beta2':0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_util(net, train_iter, val_iter, loss, trainer, ctx,\n",
    "           epochs, batch_size, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing purposes\n",
    "data = nd.reshape(sample_base, (1,3,224,224)).as_in_context(ctx)\n",
    "a = net(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
